## General matters

As with every project we will require the domain specific knowledge (we need to understand the physics behind the project) and at the end of the day the physics is what should be at the forefront of this. However, Nikolas and I seem far more interested in the method than the setting.

The result will be entirely my own work, but the approach to getting there will be a somewhat joint effort. Examiners seem to prefer if we meander towards different focusses but find a way to intertwine them, this can be awkward to do but with planning it is feasible. Same project, different focus.

Reading prior work in similar areas will be important for the early development of the project. Will have to do a literature review for the project proposal.

We must create a plan and understand our goals.

Will look like we have a far better idea of what is going on if we know what prior work has been done.

We need to be realistic in the scope.

Any program that takes over a day to run is going to be very counter productive for the running of this project. We can use google colab to run fat programs or even the universities GPUs (blue bear), also got some fancy IBM super computers lying about.

Taking a Bayesian approach, rather than a frequentist, is going to be way better. Its newer tech and it just makes more sense from a scientific point of view.

TensorFlow and TensorFlow Probability will be invaluable tools for our project.

David Hogg has done a lot of work in this field and apparently, he is a cool man, investigate this.
