# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15k2UbUuqHzCvB0AqHPVjOoW6-a8Q4mge
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_probability as tfp
import gpflow
from gpflow.utilities import print_summary, set_trainable, to_default_float
from mpl_toolkits.mplot3d import Axes3D
from gpflow.ci_utils import ci_niter

data = np.array(pd.read_csv('gyro_fake_data_v1.csv'))

Y = Y_plot = data[::10,1].reshape(-1,1)
X1 = X1_plot = data[::10,3] #rotation
X2 = X2_plot = data[::10,2] #B_V

#np.random.seed(1991)
k = gpflow.kernels.Matern12(variance=10, lengthscales=[15, 1])
#meanf = [gpflow.mean_functions.Linear(), gpflow.mean_functions.Linear()]
X = np.dstack([X1, X2]).reshape(-1, 2)
model = gpflow.models.GPR(data=(X, Y), kernel=k, mean_function=gpflow.mean_functions.Zero())#gpflow.mean_functions.Product(meanf[0], meanf[1]))
opt = gpflow.optimizers.Scipy()
opt_logs = opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=100))

x1_mesh, x2_mesh = np.meshgrid(X1, X2)

resolution = len(Y)
X1_test = np.linspace( np.min(X1), np.max(X1), num=resolution )
X2_test = np.linspace( np.min(X2), np.max(X2), num=resolution )
X1_test, X2_test = np.meshgrid( X1_test, X2_test )
X_test = np.dstack([X1_test, X2_test]).reshape(resolution, resolution, 2)

print(X1_test.shape)

# predict training set
mean, var = model.predict_y( X_test )
mean1 = np.array(mean)
mean1 = np.squeeze(mean1)
mean = tf.squeeze(mean)


num_burnin_steps = ci_niter(300)
num_samples = ci_niter(500)

print(model.trainable_parameters)

model.kernel.lengthscales.prior = tfp.distributions.Gamma(
    gpflow.utilities.to_default_float(1.0), gpflow.utilities.to_default_float(1.0)
)

# Note that here we need model.trainable_parameters, not trainable_variables - only parameters can have priors!

hmc_helper = gpflow.optimizers.SamplingHelper(
     model.log_posterior_density, model.trainable_parameters)

hmc = tfp.mcmc.HamiltonianMonteCarlo(
    target_log_prob_fn=hmc_helper, num_leapfrog_steps=10, step_size=0.01
)
adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(
    hmc, num_adaptation_steps=10, target_accept_prob=0.75, adaptation_rate=0.1
)

num_results = int(10e3)
num_burnin_steps = int(1e3)

@tf.function
def run_chain():
 samples, is_accepted = tfp.mcmc.sample_chain(
      num_results=num_results,
      num_burnin_steps=num_burnin_steps,
      current_state=1.,
      kernel=adaptive_hmc,
      trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)
 sample_mean = tf.reduce_mean(samples)
 sample_stddev = tf.math.reduce_std(samples)
 is_accepted = tf.reduce_mean(tf.cast(is_accepted, dtype=tf.float32))
 return sample_mean, sample_stddev, is_accepted



sample_mean, sample_stddev, is_accepted = run_chain()

print('mean:{:.4f}  stddev:{:.4f}  acceptance:{:.4f}'.format(
    sample_mean.numpy(), sample_stddev.numpy(), is_accepted.numpy()))

# def plot_joint_marginals(samples, parameters, y_axis_label):
#     name_to_index = {param_to_name[param]: i for i, param in enumerate(parameters)}
#     f, axs = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)

#     axs[0].plot(
#         samples[name_to_index[".likelihood.variance"]],
#         samples[name_to_index[".kernel.variance"]],
#         "k.",
#         alpha=0.15,
#     )
#     axs[0].set_xlabel("noise_variance")
#     axs[0].set_ylabel("signal_variance")

#     axs[1].plot(
#         samples[name_to_index[".likelihood.variance"]],
#         samples[name_to_index[".kernel.lengthscales"]],
#         "k.",
#         alpha=0.15,
#     )
#     axs[1].set_xlabel("noise_variance")
#     axs[1].set_ylabel("lengthscale")

#     axs[2].plot(
#         samples[name_to_index[".kernel.lengthscales"]],
#         samples[name_to_index[".kernel.variance"]],
#         "k.",
#         alpha=0.1,
#     )
#     axs[2].set_xlabel("lengthscale")
#     axs[2].set_ylabel("signal_variance")
#     f.suptitle(y_axis_label)
#     plt.show()


# plot_joint_marginals(sample_mean, model.trainable_parameters, "unconstrained variable samples")
# plot_joint_marginals(parameter_samples, model.trainable_parameters, "parameter samples")