# -*- coding: utf-8 -*-
"""stellar_sep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WsY2D6rxBNb8u8PYpGNSRkXWYQnCEFcr
"""

pip install pyAstronomy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
from mpl_toolkits.mplot3d import Axes3D
from google.colab import files
import io
from PyAstronomy import pyasl
r = pyasl.BallesterosBV_T()
b = pyasl.Ramirez2005()

uploaded = files.upload()

data0 = data = np.array(pd.read_csv(io.BytesIO(uploaded['Data1.csv'])))

def mean_fn(x, y, m, a, b, c, d):
  return ((x*1000)**a * b*(y - c)**d)*m**0.5 #the m relation was through trial and error
#fn from Barnes 2007

bv = []
d0 = []
d1 = []
d2 = []

bv_1 = []
d0_1 = []
d1_1 = []
d2_1 = []
d3 = []

for i in range(len(data[:,3])):
  if data[i, 3]<1:
    d0.append(data[i, 0]) #Teff
    d1.append(data[i, 1]) #Prot
    d2.append(data[i, 2]) #Age
    d3.append(data[i, 3]) #Mass

plt.scatter(d2, d1) #Age and Prot
plt.show()
plt.scatter(d3, d0) #Age and Teff

data = data0
amplitude = tfp.util.TransformedVariable(
  3., tfb.Exp(), dtype=tf.float64, name='amplitude')
length_scale = tfp.util.TransformedVariable(
  4., tfb.Exp(), dtype=tf.float64, name='length_scale')

X_test = np.linspace( 0, np.max(d2), num=len(d2))[..., np.newaxis]
observation_index_points = np.array(d2)[..., np.newaxis]

observations = d1

kernel = psd_kernels.MaternFiveHalves(amplitude, length_scale)
observation_noise_variance = tfp.util.TransformedVariable(
   np.exp(1), tfb.Exp(), name='observation_noise_variance')
optimizer = tf.optimizers.Adam(learning_rate=.5, beta_1=.9, beta_2=.99)

def optimize():
  with tf.GradientTape() as tape:
    loss = -gp.log_prob(observations)
  grads = tape.gradient(loss, gp.trainable_variables)
  optimizer.apply_gradients(zip(grads, gp.trainable_variables))
  return loss

gp = tfd.GaussianProcessRegressionModel(
    kernel=kernel,
    index_points=X_test,
    observation_index_points=observation_index_points,
    observations=observations, observation_noise_variance=.1)

#First train the model, then draw and plot posterior samples.
# for i in range(100):
#   neg_log_likelihood_ = optimize()
#   if i % 100 == 0:
#     print('.')

# print("Final NLL = {}".format(neg_log_likelihood_))

samples = gp.sample(10).numpy()
var = gp.variance()

plt.scatter(X_test, samples[9])
plt.scatter(d2, d1, c='k')

var = np.array(gp.variance())
print(samples[0,0].shape)
print(X1_test[0,0].shape)

numElems = len(d1)
sample = samples[0]#[0]+mean_fn(X1, X2, X3, a ,b ,c ,d)
idx = np.round(np.linspace(0, len(np.array(sample).reshape(numElems)) - 1, numElems)).astype(int)
# Picks equal spaced elements from (longer) prediction array so that its shape of data

mu_test = (np.array(sample).reshape(numElems)[idx])
sd_test = (np.array(var).reshape(numElems)[idx]) 

vals = np.sort([mu_test, sd_test], axis=1)

print(vals.shape)

plt.figure(figsize=(18,9))
print(Y.shape, X1.shape)
plt.errorbar(np.sort(d1), vals[0,:], yerr=vals[1,:], fmt='bo')
plt.scatter(np.sort(d1), vals[0,:])
x = np.linspace(0, 40)
plt.plot(x, x , 'r')

plt.xlabel('Data')
plt.ylabel('Prediction')

Z = (np.sort(d1)-vals[0,:])/vals[1,:]
print(Y.shape)
plt.figure(figsize=(9,8))
plt.hist(Z, density=True, bins=20)
mu, sigma = 0, 1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
# count, bins, ignored = plt.hist(s, 30, density=True)
# plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
#                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
#          linewidth=2, color='r', alpha=0.9)
plt.xlabel('(Data - Prediction)/$\sigma$')
plt.ylabel('Frequency')