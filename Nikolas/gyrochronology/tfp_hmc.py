# -*- coding: utf-8 -*-
"""tfp_hmc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zaw0yisBFnJEyKHke89xLJ1mp1cPyHwG
"""

pip install pyAstronomy

pip install arviz

import numpy as np
import arviz as az
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
from mpl_toolkits.mplot3d import Axes3D
from google.colab import files
import io
from PyAstronomy import pyasl
import time
r = pyasl.BallesterosBV_T()
b = pyasl.Ramirez2005()

uploaded = files.upload()

data0 = data = np.array(pd.read_csv(io.BytesIO(uploaded['SItable1.csv'])))

import tensorflow.math as tf_m
def mean_fn(x, y, a, b, c, d):
  return (np.exp(x)*1000)**a * b*(y - c)**d #the m relation was through trial and error
#fn from Barnes 2007

pd.read_csv(io.BytesIO(uploaded['SItable1.csv']))
#tolist makes array to list to remove 'dtype=float64' from the end of the array
te = data0[:,2].tolist()
tee= data0[:,3].tolist()
age = data0[:,4].tolist()
agle = data0[:,5].tolist()
ague = data0[:,6].tolist()
mass = data0[:,7].tolist()
massle = data0[:,8].tolist()
massue = data0[:,9].tolist()
p = data0[:,21].tolist()
pl = data0[:,22].tolist()
pu = data0[:,23].tolist()

mass_error = (np.array(massle) + np.array(massue))/2
age_error = (np.array(agle)+np.array(ague))/2#np.log(age) - np.log(np.array(age) - np.array(agle))
p_error = ((np.array(pl)+np.array(pu))/2)/np.array(p)#(np.log(p) - np.log(np.array(p)-np.array(pl)))

data = np.array([te, p, age, mass]).T
a = 0.5189
b=0.75
c=0.4
d=0.601
f = -0.6
X1 = np.log(data[:,2]) #age
X2 = r.t2bv(data[:,0])#data[::al,2] #B_V
X3 = data[:, 3] #mass
x = mean_fn(X1, X2, a, b, c, d)
plt.scatter(X3, x, c=X2, cmap='hsv')
plt.show()
plt.errorbar(X3, p, xerr=mass_error, yerr=p_error, fmt='o')#, c=X2, cmap='hsv')
subtract = data[:,1]-x
plt.show()
fig = plt.figure(figsize=(12, 8))
plt.scatter(X1, subtract, c=data[:,2], cmap='hsv')
plt.xlabel('Age', size=15)
plt.ylabel('Rotation', size=15)
#plt.subplots_adjust(bottom=np.min(X2), top=np.max(X2))
cax = plt.axes([0.95, 0.1, 0.075, 0.8])
plt.colorbar(cax=cax, label='B-V')
plt.show()

data_r = []
for num in range(0, len(data0)):
<<<<<<< HEAD
  if np.random.random_sample()< 0.4:
=======
  if np.random.random_sample()< 0.1:
>>>>>>> parent of a76ed96 (latent - B-V and mass)
    data_r.append(data0[num])
data_r = np.array(data_r)
print(len(data_r))

d0 = []
d1 = []
d2 = []
d3 = []

for i in range(len(data[:,3])):
  if data[i, 1]>0:
    d0.append(data[i, 0])
    d1.append(data[i, 1])
    d2.append(data[i, 2])
    d3.append(data[i, 3])

data1 = np.array([d0, d1, d2, d3]).T

tf.enable_v2_behavior()
t1 = time.perf_counter()

tfb = tfp.bijectors
tfd = tfp.distributions
psd_kernels = tfp.math.psd_kernels

# observations from a known function at some random points.
X1 = np.log(age) #age
X2 = np.log10(te)#data[::al,2] #B_V
X3 = np.array(mass)
observation_index_points = np.dstack([X1, X2, X3]).reshape(-1, 3)

resolution = len(X2)
X1_test = np.log(np.linspace( np.min(data[::,2]), np.max(data[::,2]), num=resolution ))
X2_test = np.linspace( np.min(X2), np.max(X2), num=resolution )
X3_test = np.linspace( np.min(X3), np.max(X3), num=resolution )
X_test = np.dstack([X1_test, X2_test, X3_test]).reshape(resolution,3)# resolution, resolution, 3)

a = 0.5189
b=0.75
c=0.4
d=0.601

Y = observations = (np.array(p) - mean_fn(X1, r.t2bv(np.array(te)), a, b, c, d))
noise_variance = p_error

gaussian_process_model = tfd.JointDistributionSequential([
<<<<<<< HEAD
  tfd.LogNormal(np.float64(2.), np.float64(.001)),
  tfd.LogNormal(np.float64(2.5), np.float64(.001)),
  tfd.Normal(X1.reshape(-1), (np.array(age_error)/np.array(age)).reshape(-1)),
  tfd.Normal(X2.reshape(-1), 0.43*(np.array(tee)/np.array(te)).reshape(-1)),
  tfd.Normal(X3.reshape(-1), np.array(mass_error).reshape(-1)),
  lambda amplitude, length_scale, observations1_, observations2_, observations3_: tfd.GaussianProcess(
=======
  tfd.LogNormal(np.float64(0.), np.float64(0.001)),
<<<<<<< HEAD
  tfd.LogNormal(np.float64(30.), np.float64(.01)),
  tfd.LogNormal(X1.reshape(-1), 0.43*0.2*X1.reshape(-1)),
  tfd.Normal(X2.reshape(-1), 0.04*X1.reshape(-1)),
  tfd.Normal(X3.reshape(-1), 0.04*X1.reshape(-1)),
  lambda length_scale, amplitude, observations1_, observations2_, observations3_: tfd.GaussianProcess(
>>>>>>> parent of abde0d7 (minor fixes in statistics and hmc)
      kernel=psd_kernels.ExponentiatedQuadratic(amplitude, length_scale),
      index_points=observation_index_points, observation_noise_variance=noise_variance)])

initial_chain_states = [
    2 * tf.ones([len(X1)], dtype=np.float64, name='init_amplitude'),
    2.5 * tf.ones([len(X1)], dtype=np.float64, name='init_length_scale'),
    tf.convert_to_tensor(X1.reshape(-1), dtype=np.float64, name='observations1_'),
    tf.convert_to_tensor(X2.reshape(-1), dtype=np.float64, name='observations2_'),
    tf.convert_to_tensor(X3.reshape(-1), dtype=np.float64, name='observations3_')]
=======
  tfd.LogNormal(np.float64(30.), np.float64(5.)),
  tfd.LogNormal(np.float64(1.), np.float64(1.)),
  tfd.LogNormal(X1.reshape(-1), 0.2*X1.reshape(-1)),
  lambda noise_variance, length_scale, amplitude, observations_: tfd.GaussianProcess(
      kernel=psd_kernels.ExponentiatedQuadratic(amplitude, length_scale),
      index_points=observation_index_points,
      observation_noise_variance=noise_variance)])

initial_chain_states = [
    1e0 * tf.ones([len(X1)], dtype=np.float64, name='init_amplitude'),
    30 * tf.ones([len(X1)], dtype=np.float64, name='init_length_scale'),
    1e-2 * tf.ones([len(X1)], dtype=np.float64, name='init_obs_noise_variance'),
    tf.convert_to_tensor(X1.reshape(-1), dtype=np.float64, name='observations_')]
>>>>>>> parent of a76ed96 (latent - B-V and mass)

unconstraining_bijectors = [
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus()]

def unnormalized_log_posterior(*args):
  return gaussian_process_model.log_prob(*args, x=observations)

num_results = 500
num_burnin_steps = 10000
@tf.function
def run_mcmc():
  return tfp.mcmc.sample_chain(
      num_results=num_results,
<<<<<<< HEAD
      num_burnin_steps=num_burnin_steps,
      num_steps_between_results=10,
=======
      num_burnin_steps=50,
      num_steps_between_results=3,
>>>>>>> parent of a76ed96 (latent - B-V and mass)
      current_state=initial_chain_states,
      kernel=tfp.mcmc.SimpleStepSizeAdaptation(
          inner_kernel = tfp.mcmc.HamiltonianMonteCarlo(
              target_log_prob_fn=unnormalized_log_posterior,
              step_size=[np.float64(1e-7)],
              num_leapfrog_steps=3), 
              num_adaptation_steps=int(num_burnin_steps)),
      trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)
[
      amplitudes,
      length_scales,
      observation_noise_variances,
      observations_
], is_accepted = run_mcmc()


print("Acceptance rate: {}".format(np.mean(is_accepted)))
<<<<<<< HEAD
observation_index_points = np.dstack([observations1_.numpy()[0], observations2_.numpy()[0], observations3_.numpy()[0]]).reshape(-1,3)
Y = observations = np.array(p) - mean_fn(observation_index_points[:,0], r.t2bv(10**(observation_index_points[:,1])), a, b, c, d)
=======
print(observations_.numpy()[0])
observation_index_points = np.dstack([observations_.numpy()[0], X2, X3]).reshape(-1, 3)
>>>>>>> parent of a76ed96 (latent - B-V and mass)

gp = tfd.GaussianProcessRegressionModel(
    kernel=psd_kernels.ExponentiatedQuadratic(np.mean(amplitudes), np.mean(length_scales)),
    index_points=X_test,
    observation_index_points=observation_index_points,
    observations= observations,
<<<<<<< HEAD
    observation_noise_variance=noise_variance)
=======
    observation_noise_variance=np.mean(observation_noise_variances))
>>>>>>> parent of a76ed96 (latent - B-V and mass)

#print("Final NLL = {}".format(neg_log_likelihood_))

samples = gp.sample(10).numpy()
var = np.array(gp.variance())
# ==> 10 independently drawn, joint samples at `index_points`.
# ==> 10 independently drawn, noisy joint samples at `index_points`
t2 = time.perf_counter()
print()

(observations_.numpy()[0] - X1)

acf = tfp.stats.auto_correlation(
    amplitudes, axis=-1, max_lags=None, center=True, normalize=True,
    name='auto_correlation'
)

var1 = np.array([amplitudes,
      length_scales,
      observation_noise_variances])
az.plot_autocorr(var1)
'''
, var_names=("amplitudes",
      "length_scales",
      "observation_noise_variances"))'''

numElems = len(Y)
sample = samples[0] + mean_fn(observation_index_points[:,0], r.t2bv(10**(observation_index_points[:,1])), a ,b ,c ,d)
idx = np.round(np.linspace(0, len(np.array(sample).reshape(numElems**2)) - 1, numElems)).astype(int)
# Picks equal spaced elements from (longer) prediction array so that its shape of data

mu_test = (np.array(sample).reshape(numElems**2)[idx])
sd_test = (np.array(var).reshape(numElems**2)[idx]) 

vals = np.sort([mu_test, sd_test], axis=1)

print(np.mean(Y))

font = {'size': 16,
        }
plt.figure(figsize=(18,9))
<<<<<<< HEAD
plt.errorbar(np.sort(data[::, 1]), vals[0,:], yerr=vals[1,:]**0.5, fmt='bo')
plt.fill_between(np.sort(data[::, 1]), vals[0,:] - vals[1,:]**0.5, vals[0,:] + vals[1,:]**0.5, color='blue', alpha=0.2)
plt.scatter(np.sort(data[::, 1]), np.sort(mu_test))
x = np.linspace(0, 50)
=======
#plt.errorbar(np.sort(data[::al, 1]), vals[0,:], yerr=vals[1,:]**0.5, fmt='bo')
plt.fill_between(np.sort(data[::al, 1]), vals[0,:] - 20*vals[1,:]**0.5, vals[0,:] + 20*vals[1,:]**0.5, color='blue', alpha=0.2)
plt.scatter(np.sort(data[::al, 1]), np.sort(mu_test))
x = np.linspace(0, 40)
>>>>>>> parent of a76ed96 (latent - B-V and mass)
plt.plot(x, x , 'r')
plt.xlabel('Data', fontdict=font)
plt.ylabel('Prediction', fontdict=font)

plt.figure(figsize=(12,9))
plt.errorbar(X2, np.sort(vals[0]), yerr=vals[1], fmt='o', c='y', label='Prediction')
plt.scatter(X2, np.sort(data[::al, 1]), label='Data')
plt.xlabel('Age (Gyr)', fontdict=font)
plt.ylabel('Rotation (Days)', fontdict=font)
plt.legend(fontsize='xx-large')

<<<<<<< HEAD
Z = (np.sort(data[::,1])-vals[0,:])/vals[1,:]**2
=======
Z = (np.sort(data[::al,1])-vals[0,:])/vals[1,:]**0.01
>>>>>>> parent of abde0d7 (minor fixes in statistics and hmc)
print(Y.shape)
plt.figure(figsize=(9,8))
plt.hist(Z, density=True, bins=20)
mu, sigma = 0, 1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
count, bins, ignored = plt.hist(s, 30, density=True, alpha=0.5)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
         linewidth=2, color='r', alpha=0.9)
plt.xlabel('(Data - Prediction)/$\sigma$')
plt.ylabel('Frequency')

import collections 
PooledModel = collections.namedtuple('PooledModel', ['Amplitude', 'Length_Scale'])
samples = [
      amplitudes,
      length_scales
]
pooled_samples = PooledModel._make(samples)

for var, var_samples in pooled_samples._asdict().items():
  print('R-hat for ', var, ':\t',
        tfp.mcmc.potential_scale_reduction(var_samples).numpy())

import seaborn as sns
def plot_traces(var_name, samples, num_chains):
  if isinstance(samples, tf.Tensor):
    samples = samples.numpy() # convert to numpy array
  fig, axes = plt.subplots(1, 2, figsize=(14, 1.5), sharex='col', sharey='col')
  for chain in range(num_chains):
    axes[0].plot(samples[:, chain], alpha=0.7)
    axes[0].title.set_text("'{}' trace".format(var_name))
    sns.kdeplot(samples[:, chain], ax=axes[1], shade=False)
    axes[1].title.set_text("'{}' distribution".format(var_name))
    axes[0].set_xlabel('Iteration')
    axes[1].set_xlabel(var_name)
  plt.show()

for var, var_samples in pooled_samples._asdict().items():
  plot_traces(var, samples=var_samples, num_chains=4)