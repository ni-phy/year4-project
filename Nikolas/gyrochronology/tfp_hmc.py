# -*- coding: utf-8 -*-
"""tfp_hmc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zaw0yisBFnJEyKHke89xLJ1mp1cPyHwG
"""

pip install pyAstronomy

pip install arviz

import numpy as np
import arviz as az
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
from mpl_toolkits.mplot3d import Axes3D
from google.colab import files
import io
from PyAstronomy import pyasl
r = pyasl.BallesterosBV_T()
b = pyasl.Ramirez2005()

uploaded = files.upload()

data0 = data = np.array(pd.read_csv(io.BytesIO(uploaded['Data1.csv'])))

import tensorflow.math as tf_m
def mean_fn(x, y, m, a, b, c, d, f):
  return ((x*1000)**a * b*(y - c)**d)*(m**f) #the m relation was through trial and error
#fn from Barnes 2007

data_r = []
for num in range(0, len(data0)):
  if np.random.random_sample()< 0.5:
    data_r.append(data0[num])
data_r = np.array(data_r)
print(len(data_r))

d0 = []
d1 = []
d2 = []
d3 = []

for i in range(len(data[:,3])):
  if data[i, 1]>0:
    d0.append(data[i, 0])
    d1.append(data[i, 1])
    d2.append(data[i, 2])
    d3.append(data[i, 3])

data1 = np.array([d0, d1, d2, d3]).T

data = data_r
tf.enable_v2_behavior()


tfb = tfp.bijectors
tfd = tfp.distributions
psd_kernels = tfp.math.psd_kernels

# observations from a known function at some random points.
al = 2
X1 = data[::al,2] #age
X2 = r.t2bv(data[::al,0])#data[::al,2] #B_V
X3 = data[::al, 3]
observation_index_points = np.dstack([X1, X2, X3]).reshape(-1, 3)

resolution = len(X2)
X1_test = np.linspace( np.min(X1), np.max(X1), num=resolution )
X2_test = np.linspace( np.min(X2), np.max(X2), num=resolution )
X3_test = np.linspace( np.min(X3), np.max(X3), num=resolution )
X_test = np.dstack([X1_test, X2_test, X3_test]).reshape(resolution,3)# resolution, resolution, 3)

a = 0.5189
b=0.75
c=0.4
d=0.601
f = 0.3# 0.64

Y = observations = (data[::al, 1] - mean_fn(X1, X2, X3, a, b, c, d, f))
#observations_ = tf.convert_to_tensor(X1, dtype=np.float64, name='observations_')
# amplitude = tfp.util.TransformedVariable(
#   10., tfb.Exp(), dtype=tf.float64, name='amplitude')
# length_scale = tfp.util.TransformedVariable(
#   10., tfb.Exp(), dtype=tf.float64, name='length_scale')
# kernel = psd_kernels.ExponentiatedQuadratic(amplitude, length_scale)
# observation_noise_variance = tfp.util.TransformedVariable(
#     np.exp(-5), tfb.Exp(), name='observation_noise_variance')

gaussian_process_model = tfd.JointDistributionSequential([
  tfd.LogNormal(np.float64(0.), np.float64(0.001)),
  tfd.LogNormal(np.float64(30.), np.float64(5.)),
  tfd.Normal(X1.reshape(-1), 0.2*X1.reshape(-1)),
  tfd.Normal(X2.reshape(-1), 0.04*X1.reshape(-1)),
  tfd.Normal(X3.reshape(-1), 0.04*X1.reshape(-1)),
  lambda length_scale, amplitude, observations1_, observations2_, observations3_: tfd.GaussianProcess(
      kernel=psd_kernels.ExponentiatedQuadratic(amplitude, length_scale),
      index_points=observation_index_points,
      observation_noise_variance=2)])

initial_chain_states = [
    1e0 * tf.ones([len(X1)], dtype=np.float64, name='init_amplitude'),
    30 * tf.ones([len(X1)], dtype=np.float64, name='init_length_scale'),
    tf.convert_to_tensor(X1.reshape(-1), dtype=np.float64, name='observations1_'),
    tf.convert_to_tensor(X2.reshape(-1), dtype=np.float64, name='observations2_'),
    tf.convert_to_tensor(X3.reshape(-1), dtype=np.float64, name='observations3_')]

unconstraining_bijectors = [
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus(),
    tfp.bijectors.Softplus()]

def unnormalized_log_posterior(*args):
  return gaussian_process_model.log_prob(*args, x=observations)

num_results = 500
@tf.function
def run_mcmc():
  return tfp.mcmc.sample_chain(
      num_results=num_results,
      num_burnin_steps=500,
      num_steps_between_results=3,
      current_state=initial_chain_states,
      kernel=tfp.mcmc.TransformedTransitionKernel(
          inner_kernel = tfp.mcmc.HamiltonianMonteCarlo(
              target_log_prob_fn=unnormalized_log_posterior,
              step_size=[np.float64(.00095)],
              num_leapfrog_steps=3),
          bijector=unconstraining_bijectors),
      trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)
[
      amplitudes,
      length_scales,
      observations1_,
      observations2_, observations3_
], is_accepted = run_mcmc()

print("Acceptance rate: {}".format(np.mean(is_accepted)))
print(observations_.numpy()[0])
observation_index_points = np.dstack([observations1_.numpy()[0], observations2_.numpy()[0], observations3_.numpy()[0]]).reshape(-1,3)

gp = tfd.GaussianProcessRegressionModel(
    kernel=psd_kernels.ExponentiatedQuadratic(np.mean(amplitudes), np.mean(length_scales)),
    index_points=X_test,
    observation_index_points=observation_index_points,
    observations= observations,
    observation_noise_variance=2)

#print("Final NLL = {}".format(neg_log_likelihood_))

samples = gp.sample(10).numpy()
var = np.array(gp.variance())
# ==> 10 independently drawn, joint samples at `index_points`.
# ==> 10 independently drawn, noisy joint samples at `index_points`

acf = tfp.stats.auto_correlation(
    amplitudes, axis=-1, max_lags=None, center=True, normalize=True,
    name='auto_correlation'
)

var1 = np.array([amplitudes,
      length_scales])
az.plot_autocorr(var1)
'''
, var_names=("amplitudes",
      "length_scales",
      "observation_noise_variances"))'''

numElems = len(Y)
sample = samples[0] + mean_fn(X1, X2, X3, a ,b ,c ,d, f)
idx = np.round(np.linspace(0, len(np.array(sample).reshape(numElems)) - 1, numElems)).astype(int)
# Picks equal spaced elements from (longer) prediction array so that its shape of data

mu_test = (np.array(sample).reshape(numElems)[idx])
sd_test = (np.array(var).reshape(numElems)[idx]) 

vals = np.sort([mu_test, sd_test], axis=1)

print(np.mean(Y))
print(np.mean(observation_noise_variances))

plt.figure(figsize=(18,9))
#plt.errorbar(np.sort(data[::al, 1]), vals[0,:], yerr=vals[1,:]**0.5, fmt='bo')
plt.fill_between(np.sort(data[::al, 1]), vals[0,:] - vals[1,:]**0.5, vals[0,:] + vals[1,:]**0.5, color='blue', alpha=0.2)
plt.scatter(np.sort(data[::al, 1]), np.sort(mu_test))
x = np.linspace(0, 40)
plt.plot(x, x , 'r')
plt.xlabel('Data')
plt.ylabel('Prediction')

Z = (np.sort(data[::al,1])-vals[0,:])/vals[1,:]**0.01
print(Y.shape)
plt.figure(figsize=(9,8))
plt.hist(Z, density=True, bins=20)
mu, sigma = 0, 1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
count, bins, ignored = plt.hist(s, 30, density=True, alpha=0.5)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
         linewidth=2, color='r', alpha=0.9)
plt.xlabel('(Data - Prediction)/$\sigma$')
plt.ylabel('Frequency')

