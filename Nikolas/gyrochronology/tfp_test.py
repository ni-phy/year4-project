# -*- coding: utf-8 -*-
"""tfp_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r2CFF7Zfwj8muLWg90KNDA_fnIK3gqBj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
from mpl_toolkits.mplot3d import Axes3D
# from google.colab import files
# import io

# uploaded = files.upload()

def mean_fn(x, y, a, b, c, d):
  return (((x*1000)**a) * b*(y - c)**d)
#fn from Barnes 2007

#rouch check the fn works
a = 0.5189
b=0.75
c=0.4
d=0.601
y = data[:,3]
x = mean_fn(data[:,1], data[:,2], a, b, c ,d)
plt.scatter(data[:,1], x, c=data[:,2], cmap='hsv')
print(np.mean(x), np.mean(y), np.mean(x-y))
plt.show()
plt.scatter(data[:,1], data[:,3], c=data[:,2], cmap='hsv')

#data = np.array(pd.read_csv(io.BytesIO(uploaded['gyro_fake_data_v1.csv'])))
data = np.array(pd.read_csv('gyro_fake_data_v1.csv'))

tf.enable_v2_behavior()

tfb = tfp.bijectors
tfd = tfp.distributions
psd_kernels = tfp.math.psd_kernels

# observations from a known function at some random points.
a=3
X1 = X1_plot = data[::a,1] #rotation
X2 = X2_plot = data[::a,2] #B_V
X = np.dstack([X1, X2]).reshape(-1, 2)

x1_mesh, x2_mesh = np.meshgrid(X1, X2)

resolution = len(Y)
X1_test = np.linspace( np.min(X1), np.max(X1), num=resolution )
X2_test = np.linspace( np.min(X2), np.max(X2), num=resolution )
X1_test, X2_test = np.meshgrid( X1_test, X2_test )
X_test = np.dstack([X1_test, X2_test]).reshape(resolution, resolution, 2)
Y = (data[::al, 3] - mean_fn(X1, X2, a, b, c, d)).reshape(-1,1)

print(observation_index_points.shape, observations.shape)
amplitude = tfp.util.TransformedVariable(
  10., tfb.Exp(), dtype=tf.float64, name='amplitude')
length_scale = tfp.util.TransformedVariable(
  10., tfb.Exp(), dtype=tf.float64, name='length_scale')
kernel = psd_kernels.ExponentiatedQuadratic(
  amplitude,length_scale)*psd_kernels.ExponentiatedQuadratic(
    amplitude, length_scale=3)

observation_index_points = X 
observations = Y.T
kernel = psd_kernels.ExponentiatedQuadratic(amplitude, length_scale=10)* psd_kernels.ExponentiatedQuadratic(amplitude, length_scale=1)

observation_noise_variance = tfp.util.TransformedVariable(
   np.exp(0), tfb.Exp(), name='observation_noise_variance')

optimizer = tf.optimizers.Adam(learning_rate=.05, beta_1=.5, beta_2=.99)

@tf.function
def optimize():
  with tf.GradientTape() as tape:
    loss = -gp.log_prob(observations)
  grads = tape.gradient(loss, gp.trainable_variables)
  optimizer.apply_gradients(zip(grads, gp.trainable_variables))
  return loss

gp = tfd.GaussianProcessRegressionModel(
    kernel=kernel,
    index_points=X_test,
    observation_index_points=X,
    observations=Y.T, mean_fn=mean_fn)

for i in range(1000):
  neg_log_likelihood_ = optimize()
  if i % 100 == 0:
    print("Step {}: NLL = {}".format(i, neg_log_likelihood_))

samples = gp.sample(10)
var = gp.variance()
# ==> 10 independently drawn, joint samples at `index_points`.
# ==> 10 independently drawn, noisy joint samples at `index_points`

fig = plt.figure(figsize=(18, 10))
ax = plt.axes(projection='3d')
ax.view_init(0, 40)
ax.plot_surface(X1_test, X2_test, samples[0], antialiased=True, alpha=0.7, linewidth=0.5, cmap='winter')
ax.scatter3D(X1, X2, Y, marker='o',edgecolors='k', color='r', s=150)
ax.set_xlabel('B-V Index')
ax.set_ylabel('Rotation Period (Days)')
ax.set_zlabel('Age (Gyr)')

plt.show()

plt.scatter(X1_test, samples[0]+mean_fn(X1_test, X2_test, a, b, c ,d)
            , cmap='hsv', c=X2_test)
plt.show() #checking the data looks like prediction

numElems = len(Y)
idx = np.round(np.linspace(0, len(np.array(samples[9]).reshape(numElems**2)) - 1, numElems)).astype(int)
# Picks equal spaced elements from (longer) prediction array so that its shape of data

mu_test = (np.array(samples[9]).reshape(numElems**2)[idx])
sd_test = (np.array(var).reshape(numElems**2)[idx]) 

vals = np.sort([mu_test, sd_test], axis=1)

print(vals.shape)

plt.figure(figsize=(18,9))
print(Y.shape, X1.shape)
plt.errorbar(np.sort(data[::al,3]), vals[0,:], yerr=vals[1,:]**2, fmt='bo')
#plt.scatter(Y+mean_fn(data[::al,1], data[::al,2], a, b, c, d).reshape(-1,1), vals[0,:])
plt.plot(np.linspace( np.min(vals[0,:]), np.max(vals[0,:]), num=resolution ), 
         np.linspace( np.min(vals[0,:]), np.max(vals[0,:]), num=resolution ), 'r')
#plt.ylim((0,20))
plt.xlabel('Data')
plt.ylabel('Prediction')

#checking the std
Z = (np.sort(data[::3,1])-vals[0,:])/vals[1,:]
plt.hist(Z, density=True, bins=8)
mu, sigma = 0, 1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)
count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
         linewidth=2, color='r')